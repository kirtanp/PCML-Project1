{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from proj1_helpers import *\n",
    "from costs import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "DATA_TRAIN_PATH = '../Data/train.csv' # TODO: add a file Data-Project1 with the train data \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete the outliers with the median\n",
    "def delete_outliers(tX):\n",
    "    for idx_feature in range(tX.shape[1]):\n",
    "        tX_feature = tX[:,idx_feature]\n",
    "        median = np.median(tX_feature[np.where(tX_feature != -999)])\n",
    "        new = np.where(tX_feature == -999, median, tX_feature)\n",
    "        tX[:, idx_feature] = np.copy(new)\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX = delete_outliers(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "stx, mean_stx, std_x = standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y must be 0 or 1 and not -1 or 1\n",
    "def set_y(y):\n",
    "    y = np.where(y == -1, 0, y)\n",
    "    return y\n",
    "y = set_y(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of the Features that do not provide any more information than the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArkAAAGeCAYAAACQHxmnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmGjtzkJkmSJEmSJEPH/wBzvIP89Wm9JgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d1d441470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_f = 2\n",
    "signal = stx[np.where(y == 1), idx_f + 1]\n",
    "background = stx[np.where(y == 0), idx_f + 1]\n",
    "\n",
    "plot = plt.figure()\n",
    "plt.boxplot([signal, background], 0, '')\n",
    "plt.xticks([1, 2], ['Signal', 'Background'])\n",
    "plot.suptitle('Signal and Background distribution of Feature {f}'.format(f = idx_f + 1))\n",
    "\n",
    "textvar = plot.text(0, 0, 'If the plot is similar, it means the signal does not provide more information than the background.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_correlation(stx):\n",
    "    corr = np.ones((stx.shape[1]-1, stx.shape[1]-1))\n",
    "    for feature1 in range(1, stx.shape[1]):\n",
    "        for feature2 in range(1, stx.shape[1]):\n",
    "            corr[feature1-1, feature2-1] = np.corrcoef(stx[:, feature1], stx[:, feature2])[0, 1]\n",
    "            if (corr[feature1-1, feature2-1] >= 0.9 and feature1-1 != feature2-1):\n",
    "                \n",
    "                print(\"Features {f1} and {f2} are highly correlated: {corr}\".format(f1 =feature1-1, f2 = feature2-1, corr = corr[feature1-1, feature2-1]))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features 9 and 21 are highly correlated: 0.9044814595684958\n",
      "Features 9 and 29 are highly correlated: 0.9656283889163997\n",
      "Features 21 and 9 are highly correlated: 0.9044814595684957\n",
      "Features 29 and 9 are highly correlated: 0.9656283889163997\n"
     ]
    }
   ],
   "source": [
    "corr = calculate_correlation(stx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_to_del = np.array([22, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_correlation_with_y(stx, y, threshold):\n",
    "    corr = np.ones(stx.shape[1]-1)\n",
    "    for feature in range(1, stx.shape[1]):\n",
    "        corr[feature-1] = np.corrcoef(y, stx[:, feature])[0, 1] \n",
    "        if (abs(corr[feature-1]) <= threshold):\n",
    "            print(\"feature {f} is not correlated with y: {corr}\".format(f = feature-1, corr= corr[feature-1]))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 14 is not correlated with y: -0.0009432510582117535\n",
      "feature 15 is not correlated with y: -0.00440253868638843\n",
      "feature 17 is not correlated with y: 0.001516235377059733\n",
      "feature 18 is not correlated with y: 0.00412544741152486\n",
      "feature 24 is not correlated with y: 7.15909820593841e-05\n",
      "feature 25 is not correlated with y: 0.0009043288374294407\n",
      "feature 27 is not correlated with y: 0.0005721318815439868\n",
      "feature 28 is not correlated with y: -0.003524581655021693\n"
     ]
    }
   ],
   "source": [
    "corr = calculate_correlation_with_y(stx, y, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_to_del = np.append(idx_to_del, [ 15, 16, 18, 19, 25, 26, 28, 29])\n",
    "#idx_to_del = np.append(idx_to_del, [ 15, 16, 18, 19, 21, 25, 26, 28, 29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 29 28 26 25 22 19 18 16 15]\n"
     ]
    }
   ],
   "source": [
    "idx_to_del = np.sort(idx_to_del)\n",
    "idx_to_del = idx_to_del[::-1]\n",
    "print(idx_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_features(stx):\n",
    "    return np.delete(stx, idx_to_del, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0263822 , -0.35142796, -0.01405527,  0.19252633,  0.20347244,\n",
       "        0.21249463, -0.18361342,  0.01224548, -0.01528743,  0.15323593,\n",
       "       -0.1953979 ,  0.27175188,  0.17514537,  0.23523798, -0.03194759,\n",
       "        0.02246575,  0.00747534,  0.13354912,  0.11545223,  0.02287271])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tx = delete_features(np.copy(stx))\n",
    "calculate_correlation_with_y(clean_tx, y, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    dim = degree+1\n",
    "    N = x.shape[0]\n",
    "    phi = np.ones((N, x.shape[1]))\n",
    "    for j in range(1, dim):\n",
    "        phi = np.concatenate((phi, np.power(x,j)), axis =1)\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    1.,    0.,    9.,    0.,   81.,    0.,  729.],\n",
       "       [   1.,    1.,    1.,    8.,    1.,   64.,    1.,  512.],\n",
       "       [   1.,    1.,    2.,    7.,    4.,   49.,    8.,  343.],\n",
       "       [   1.,    1.,    3.,    6.,    9.,   36.,   27.,  216.],\n",
       "       [   1.,    1.,    4.,    5.,   16.,   25.,   64.,  125.],\n",
       "       [   1.,    1.,    5.,    4.,   25.,   16.,  125.,   64.],\n",
       "       [   1.,    1.,    6.,    3.,   36.,    9.,  216.,   27.],\n",
       "       [   1.,    1.,    7.,    2.,   49.,    4.,  343.,    8.],\n",
       "       [   1.,    1.,    8.,    1.,   64.,    1.,  512.,    1.],\n",
       "       [   1.,    1.,    9.,    0.,   81.,    0.,  729.,    0.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0,1,2,3,4,5,6,7,8,9],[9,8,7,6,5,4,3,2,1,0]])\n",
    "build_poly(a.T, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 84)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 3\n",
    "poly_tx = build_poly(clean_tx, degree)\n",
    "poly_tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Prediction estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def divide_training_data(y, x, ratio ):\n",
    "    \"\"\"Divide a dataset into 2 disjoint parts. We will use this on the training data set \n",
    "    so that we can train on one and test on the other to check the accuracy of our prediction\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(np.arange(len(y)), int( len(y)*ratio ), replace=False)\n",
    "    \n",
    "    # training data \n",
    "    x_train = x[indices]\n",
    "    y_train = y[indices]\n",
    "    \n",
    "    # test data \n",
    "    x_test = x[~indices]\n",
    "    y_test = y[~indices]\n",
    "    \n",
    "\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prediction( y, tX, gamma, max_iters, lambda_  ):\n",
    "    # divide data\n",
    "    x_train, x_test, y_train, y_test = divide_training_data(y, tX, 0.7 )\n",
    "    \n",
    "    loss = 0\n",
    "    w = []\n",
    "    loss, w = reg_logistic_regression( y_train, x_train, lambda_, gamma, max_iters )\n",
    "    \n",
    "    y_pred = predict_labels(w, x_test)    \n",
    "    y_pred = set_y(y_pred)\n",
    "    \n",
    "    # accuracy of the prediction\n",
    "    N = y_test.shape\n",
    "    print(N, y_pred.shape)\n",
    "    pred = np.sum(y_pred == y_test)/N[0]\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=[[ 121300.75659799]]\n",
      "Current iteration=100, the loss=[ 92591.37756768]\n",
      "Current iteration=200, the loss=[ 87980.84054326]\n",
      "Current iteration=300, the loss=[ 85604.37461214]\n",
      "Current iteration=400, the loss=[ 94019.85737978]\n",
      "Current iteration=500, the loss=[ 85739.60114351]\n",
      "Current iteration=600, the loss=[ 89969.06282963]\n",
      "Current iteration=700, the loss=[ 88951.76534737]\n",
      "Current iteration=800, the loss=[ 87822.10492949]\n",
      "Current iteration=900, the loss=[ 89612.47918474]\n",
      "(175000,) (175000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79447428571428569"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(y, poly_tx,  1e-7, 1000, 0.05 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - tx @ w\n",
    "    return -(tx.T @ e)/len(y)\n",
    "\n",
    "def linear_regression_GD(y, tx, gamma, max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    ws = [np.copy(w)]\n",
    "    losses = []\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient with mse\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        \n",
    "        # update w\n",
    "        w = w - (gamma*grad)\n",
    "        \n",
    "        # calculate the cost with mse\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    print(\"Optimal weights: {w}\".format(w=ws[tx.shape[1]]), \"\\n Loss: {loss}\".format(loss = loss))\n",
    "    return ( loss, w )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss for Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: [  1.04648969e-02   3.75362625e-04  -4.99656749e-03  -1.91176313e-04\n",
      "   2.60325480e-03   2.83333842e-03   2.92009580e-03  -2.53973496e-03\n",
      "   2.92672884e-04  -2.91997173e-04   2.02535218e-03  -2.78308290e-03\n",
      "   3.80261289e-03   2.46044555e-03   3.29369359e-03  -1.44874300e-05\n",
      "  -6.42817694e-05  -4.79514601e-04   1.95107674e-05   5.97363034e-05\n",
      "   2.21582085e-04   1.05131123e-04   1.77564564e-03   1.77070788e-03\n",
      "   1.50527857e-03   9.97254502e-07   1.20346588e-05   2.19947304e-04\n",
      "   8.90938540e-06  -4.90302043e-05   1.74571674e-03] \n",
      " Loss: 0.15160026197005938\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.001\n",
    "max_iters = 100\n",
    "losses, ws = linear_regression_GD(y, stx, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_regression_SGD(y, tx, gamma, max_iters):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    ws = [np.copy(w)]\n",
    "    losses = []\n",
    "    batch_size = 50000\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient for each ini batch\n",
    "        a = 0;\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=1):\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            \n",
    "            # upgrade w\n",
    "            w = w - (gamma*grad)\n",
    "\n",
    "            # compute loss with mse\n",
    "            loss = compute_loss(y, tx, w)\n",
    "\n",
    "            # store w and loss\n",
    "            ws.append(np.copy(w))\n",
    "            losses.append(loss)\n",
    "    print(\"Optimal weights: {w}\\n\\n Loss: {l}\".format(w=ws[tx.shape[1]], l = loss))\n",
    "    return loss, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: [  1.04705105e-02   3.79845674e-04  -4.97651176e-03  -1.94235207e-04\n",
      "   2.63344112e-03   2.81190678e-03   2.90083034e-03  -2.52081839e-03\n",
      "   2.80011928e-04  -2.86502247e-04   2.03850299e-03  -2.76886684e-03\n",
      "   3.82063932e-03   2.45931692e-03   3.29498777e-03  -2.14715757e-05\n",
      "  -6.96247980e-05  -4.62731675e-04   2.84074841e-05   6.06792421e-05\n",
      "   2.59777250e-04   1.27353977e-04   1.78733661e-03   1.77304161e-03\n",
      "   1.52269183e-03   1.33327211e-05   7.02572929e-06   2.16524382e-04\n",
      "   4.10631762e-06  -6.96207410e-05   1.75716700e-03]\n",
      "\n",
      " Loss: 0.15160312268989676\n"
     ]
    }
   ],
   "source": [
    "losses, ws = linear_regression_SGD(y, stx, gamma, max_iters)\n",
    "w_LSSGD = ws[-1]\n",
    "w_LSSGD = w_LSSGD[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.42668000e-01,   6.11361314e-03,  -1.26533292e-01,\n",
       "         -1.31728099e-01,   6.46217226e-03,   9.64026088e-03,\n",
       "          5.20573141e-02,   3.57195511e-03,   1.40111528e-01,\n",
       "         -1.38806755e-02,  -1.60323202e+02,  -9.38377506e-02,\n",
       "          6.00765800e-02,   3.72963099e-02,   3.11465307e+01,\n",
       "         -4.00142227e-04,  -4.05770496e-04,   3.07170376e+01,\n",
       "         -3.26073105e-04,   1.27894398e-03,   5.01947429e-02,\n",
       "          4.71663263e-04,  -2.39219337e-02,   2.73366768e-02,\n",
       "         -1.86624775e-02,   2.92313230e-04,   1.24560590e-04,\n",
       "         -9.10599047e-03,   7.58088080e-04,  -8.07604911e-04,\n",
       "          1.35788484e+02]), 0.084989533074598111)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    w = np.linalg.solve(tx.T @ tx, tx.T @ y)\n",
    "    return w, compute_loss(y, tx, w)\n",
    "\n",
    "least_squares(y, stx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0804883654696 [  1.56726458e-02   1.56726458e-02   1.56726458e-02   1.56726458e-02\n",
      "   1.56726458e-02   1.56726458e-02   1.56726458e-02   1.56726457e-02\n",
      "   1.56726457e-02   1.56726457e-02   1.56726457e-02   1.56726457e-02\n",
      "   1.56726457e-02   1.56726457e-02   1.56726457e-02   1.56726457e-02\n",
      "   1.56726457e-02   1.56726457e-02   1.56726457e-02   1.56726458e-02\n",
      "   7.14113814e-02  -1.27044839e-01  -1.69367162e-01   6.52987288e-02\n",
      "   3.15068257e-02   8.36787405e-03   3.20595673e-02   1.35198187e-01\n",
      "  -5.27702483e-03  -3.27233246e-02  -9.95378771e-02   2.06527254e-02\n",
      "   3.85748775e-02   1.09789838e-01   1.33741205e-01   9.69723532e-03\n",
      "  -7.83463060e-03   2.21650189e-02   1.25279975e-02   1.56726458e-02\n",
      "  -9.81925605e-03   1.83773578e-02   8.97138450e-03  -2.98324741e-04\n",
      "   2.39493400e-02  -9.86790493e-04  -3.33297567e-03  -3.25166325e-02\n",
      "   4.52597746e-04  -5.34438978e-03   9.61532709e-03   3.04258916e-02\n",
      "   1.05091740e-02  -8.09973375e-03  -1.05490955e-02  -1.28285789e-03\n",
      "  -2.88630621e-02  -3.42322691e-03   8.50592939e-05]\n"
     ]
    }
   ],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    w = np.linalg.solve((tx.T @ tx) + lamb*np.eye(tx.shape[1]), tx.T @ y)\n",
    "    return w, compute_loss(y, tx, w)\n",
    "\n",
    "lamb = 0\n",
    "weights, loss = ridge_regression(y, poly_tx, 0.1)\n",
    "w = weights[1:]\n",
    "print(loss, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    # equivalent to use 1/(1+exp(-t)) but avoids overflow\n",
    "    return np.exp(-np.logaddexp(0, -t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    #for n in range(N):\n",
    "    #   cost += np.log(1+np.exp(tx[n, :].T @ w)) - (y[n] * tx[n, :].T @ w)\n",
    "    y = y.reshape((-1, 1))\n",
    "    return np.sum(np.logaddexp(0, tx @ w)) - y.T @ (tx @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    sig = sigmoid(tx @ w)\n",
    "    sig = sig.reshape(sig.shape[0],)\n",
    "    return tx.T @ (sig - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_* (w.T @ w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    \n",
    "    w.shape = (w.shape[0],)\n",
    "    w = w - gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, gamma, max_iters):\n",
    "    \"\"\"\n",
    "    Logistic regression using GD\n",
    "    \"\"\"\n",
    "    # init parameters\n",
    "    threshold = 0.000001\n",
    "    losses = []\n",
    "\n",
    "    # build w\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    # y must be 0 or 1 and not -1 or 1(as implemented in the lab)\n",
    "    y = np.where(y == -1, 0, y)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma, lambda_ = 0)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria ( max_iters is really high)\n",
    "        losses.append(np.copy(loss))\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=[[ 173286.79513999]]\n",
      "Current iteration=100, the loss=[ 127154.28072546]\n",
      "Current iteration=200, the loss=[ 126618.79402675]\n",
      "Current iteration=300, the loss=[ 126464.35203379]\n",
      "Current iteration=400, the loss=[ 126413.43974288]\n",
      "Current iteration=500, the loss=[ 126395.1711419]\n",
      "Current iteration=600, the loss=[ 126388.06668287]\n",
      "Current iteration=700, the loss=[ 126385.07913351]\n",
      "Current iteration=800, the loss=[ 126383.72395774]\n",
      "Current iteration=900, the loss=[ 126383.06264566]\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10000\n",
    "gamma = 0.000001\n",
    "loss_LR, w_LR = logistic_regression(y, poly_tx, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized  logistic  regression  using  gradient  descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, gamma, max_iters):\n",
    "    \"\"\"\n",
    "    Logistic regression using GD\n",
    "    \"\"\"\n",
    "    # init parameters\n",
    "    threshold = 1e-4\n",
    "    losses = []\n",
    "\n",
    "    # build w\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria ( max_iters is really high)\n",
    "        losses.append(np.copy(loss))\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=[[ 173286.79513999]]\n",
      "Current iteration=100, the loss=[ 141323.29314839]\n",
      "Current iteration=200, the loss=[ 219165.80385852]\n",
      "Current iteration=300, the loss=[ 140069.35426803]\n",
      "Current iteration=400, the loss=[ 145746.1922003]\n",
      "Current iteration=500, the loss=[ 136910.76486583]\n",
      "Current iteration=600, the loss=[ 136533.62539239]\n",
      "Current iteration=700, the loss=[ 136305.44432451]\n",
      "Current iteration=800, the loss=[ 131669.93980288]\n",
      "Current iteration=900, the loss=[ 131122.87487126]\n",
      "Current iteration=1000, the loss=[ 127021.54606952]\n",
      "Current iteration=1100, the loss=[ 123287.66436529]\n",
      "Current iteration=1200, the loss=[ 119477.28381908]\n",
      "Current iteration=1300, the loss=[ 130379.4747004]\n",
      "Current iteration=1400, the loss=[ 136867.46784882]\n",
      "Current iteration=1500, the loss=[ 129828.87757552]\n",
      "Current iteration=1600, the loss=[ 135338.13974367]\n",
      "Current iteration=1700, the loss=[ 143708.75796054]\n",
      "Current iteration=1800, the loss=[ 157296.43221419]\n",
      "Current iteration=1900, the loss=[ 131804.47863799]\n",
      "Current iteration=2000, the loss=[ 131649.57962284]\n",
      "Current iteration=2100, the loss=[ 121528.6510634]\n",
      "Current iteration=2200, the loss=[ 134143.23082241]\n",
      "Current iteration=2300, the loss=[ 117547.55704504]\n",
      "Current iteration=2400, the loss=[ 135375.12694828]\n",
      "Current iteration=2500, the loss=[ 131442.43374648]\n",
      "Current iteration=2600, the loss=[ 143373.79319256]\n",
      "Current iteration=2700, the loss=[ 156095.43764767]\n",
      "Current iteration=2800, the loss=[ 172007.5543744]\n",
      "Current iteration=2900, the loss=[ 120733.20439857]\n",
      "Current iteration=3000, the loss=[ 120041.85209259]\n",
      "Current iteration=3100, the loss=[ 118882.30864369]\n",
      "Current iteration=3200, the loss=[ 128435.63985805]\n",
      "Current iteration=3300, the loss=[ 148749.01453753]\n",
      "Current iteration=3400, the loss=[ 132056.93696084]\n",
      "Current iteration=3500, the loss=[ 159631.33443631]\n",
      "Current iteration=3600, the loss=[ 134667.4743822]\n",
      "Current iteration=3700, the loss=[ 123483.8240054]\n",
      "Current iteration=3800, the loss=[ 134519.37120328]\n",
      "Current iteration=3900, the loss=[ 130972.07373378]\n",
      "Current iteration=4000, the loss=[ 128288.56857962]\n",
      "Current iteration=4100, the loss=[ 131154.22642889]\n",
      "Current iteration=4200, the loss=[ 143446.9902208]\n",
      "Current iteration=4300, the loss=[ 140598.24119525]\n",
      "Current iteration=4400, the loss=[ 122101.95279087]\n",
      "Current iteration=4500, the loss=[ 122375.610546]\n",
      "Current iteration=4600, the loss=[ 156158.74908136]\n",
      "Current iteration=4700, the loss=[ 129369.208945]\n",
      "Current iteration=4800, the loss=[ 121309.87987006]\n",
      "Current iteration=4900, the loss=[ 131544.01135348]\n",
      "Current iteration=5000, the loss=[ 120931.46334386]\n",
      "Current iteration=5100, the loss=[ 125219.34112401]\n",
      "Current iteration=5200, the loss=[ 130669.30248815]\n",
      "Current iteration=5300, the loss=[ 125838.33245141]\n",
      "Current iteration=5400, the loss=[ 204686.70463482]\n",
      "Current iteration=5500, the loss=[ 134777.73569709]\n",
      "Current iteration=5600, the loss=[ 119539.55810372]\n",
      "Current iteration=5700, the loss=[ 133635.53453451]\n",
      "Current iteration=5800, the loss=[ 122067.97314108]\n",
      "Current iteration=5900, the loss=[ 140850.80280869]\n",
      "Current iteration=6000, the loss=[ 143739.85662078]\n",
      "Current iteration=6100, the loss=[ 129381.97153078]\n",
      "Current iteration=6200, the loss=[ 114367.6542135]\n",
      "Current iteration=6300, the loss=[ 120293.20857478]\n",
      "Current iteration=6400, the loss=[ 140366.48613549]\n",
      "Current iteration=6500, the loss=[ 131285.59880779]\n",
      "Current iteration=6600, the loss=[ 114978.27183527]\n",
      "Current iteration=6700, the loss=[ 129740.13873095]\n",
      "Current iteration=6800, the loss=[ 127784.13001029]\n",
      "Current iteration=6900, the loss=[ 131122.18965138]\n",
      "Current iteration=7000, the loss=[ 133101.64360021]\n",
      "Current iteration=7100, the loss=[ 136163.28662229]\n",
      "Current iteration=7200, the loss=[ 127859.74610441]\n",
      "Current iteration=7300, the loss=[ 137441.67687338]\n",
      "Current iteration=7400, the loss=[ 138612.74407975]\n",
      "Current iteration=7500, the loss=[ 131010.16923656]\n",
      "Current iteration=7600, the loss=[ 123871.47926101]\n",
      "Current iteration=7700, the loss=[ 131173.82825643]\n",
      "Current iteration=7800, the loss=[ 130185.84945978]\n",
      "Current iteration=7900, the loss=[ 152058.75525215]\n",
      "Current iteration=8000, the loss=[ 127829.12505076]\n",
      "Current iteration=8100, the loss=[ 119079.46814671]\n",
      "Current iteration=8200, the loss=[ 131216.87671518]\n",
      "Current iteration=8300, the loss=[ 121960.52958251]\n",
      "Current iteration=8400, the loss=[ 172744.28752004]\n",
      "Current iteration=8500, the loss=[ 121151.14020982]\n",
      "Current iteration=8600, the loss=[ 134874.84326951]\n",
      "Current iteration=8700, the loss=[ 131665.60047586]\n",
      "Current iteration=8800, the loss=[ 169050.45577958]\n",
      "Current iteration=8900, the loss=[ 128227.81657758]\n",
      "Current iteration=9000, the loss=[ 113537.26914057]\n",
      "Current iteration=9100, the loss=[ 131882.09770803]\n",
      "Current iteration=9200, the loss=[ 127735.49931621]\n",
      "Current iteration=9300, the loss=[ 116873.68377317]\n",
      "Current iteration=9400, the loss=[ 203388.8187644]\n",
      "Current iteration=9500, the loss=[ 155953.04817281]\n",
      "Current iteration=9600, the loss=[ 141776.23461284]\n",
      "Current iteration=9700, the loss=[ 118858.65285244]\n",
      "Current iteration=9800, the loss=[ 118002.69630565]\n",
      "Current iteration=9900, the loss=[ 142432.65123082]\n"
     ]
    }
   ],
   "source": [
    "gamma = 1e-7\n",
    "max_iters = 10000\n",
    "lambda_ = 0.07\n",
    "loss, w = reg_logistic_regression(y, poly_tx, lambda_, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the minimum of rmse_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cross_validation import *\n",
    "seed = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-4, 2, 30)\n",
    "cross_validation_demo(y, stx, k_fold, lambdas, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../Data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_test = delete_outliers(tX_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stx_test, mean_stx_test, std_x_test = standardize(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_test = delete_features(stx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 84)\n"
     ]
    }
   ],
   "source": [
    "poly_test = build_poly(clean_test, degree)\n",
    "print(poly_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../Data/Data_submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, poly_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
