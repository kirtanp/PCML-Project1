{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from proj1_helpers import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "DATA_TRAIN_PATH = '../Data/train.csv' # TODO: add a file Data-Project1 with the train data \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete the outliers with the median\n",
    "def delete_outliers(tX):\n",
    "    for idx_feature in range(tX.shape[1]):\n",
    "        tX_feature = tX[:,idx_feature]\n",
    "        median = np.median(tX_feature[np.where(tX_feature != -999)])\n",
    "        new = np.where(tX_feature == -999, median, tX_feature)\n",
    "        tX[:, idx_feature] = np.copy(new)\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX = delete_outliers(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "stx, mean_stx, std_x = standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y must be 0 or 1 and not -1 or 1\n",
    "def set_y(y):\n",
    "    y = np.where(y == -1, 0, y)\n",
    "    return y\n",
    "y = set_y(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of the Features that do not provide any more information than the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArkAAAGeCAYAAACQHxmnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3XmcHXWd7//XWxBDEOOoQ4hXFlEwHRA0ccMNFBXFK+I2\n3iiCjjIu4+gvOldHx5Gg4yBucXTcBhX1h+bKXHdHRUUUYUS0gwKhIyjBUQmLgmFJgki+94+qhspJ\n9+kl3X2667yej8d5nD51vvWtT+2f861vVaeUgiRJktQmd+l1AJIkSdJUM8mVJElS65jkSpIkqXVM\nciVJktQ6JrmSJElqHZNcSZIktY5JriRJklrHJFeSJEmtY5IrSZKk1jHJ1bRLsjXJ0T2Y7vokr+nB\ndA+r5/keMz3tjjj2qeM4uJdxTLUkxye5YYLjbLMs6nV0+3Sso+b2Pt3roFf71ghxLEzynSQ3J7m+\n1/FIEpjkagcluU+SjyT5dZItSTYk+WaSQxvF9gS+2asYe6Tr/8tOcmWdoGxN8uckv0vy8ST3nMk4\n5rDJzFdznPOARaWUG8caaRI/Wjq39x1eB0lOTHLhOKbVKyuAhcDBwAEjFajnYWv942Jr4+8nTlUQ\nXZbTjEnyrCQ/SXJDnfRfmOTYXsYk9audex2A5rwvUm1HLwLWU53ojgDuPVyglHJtb0Kb1QrwFuDj\nwE5UicGpwL8Cx0/hdDKFdVUVJjuXUv481fXOgDuWRR3/eLfLUK2vrssyyV1LKbeNsL1P1TrYLlme\nRfvWA4DBUsoVY5S7hOr40FwmU93yOyU/7JLsVEq5fRKj/gH4Z2Ad8CfgGcBpSa4ppXxnKmKTND62\n5GrSkiwAHgu8sZRyTinlN6WUn5ZSTimlfL1RbptLqkkeXbdubE5yfpJnjHApeWuSJ9YtIrckOS/J\nAY069kvy5SRXJ7kpyQVJjphg/A9L8u0k1yX5Y5LvJ3loR5mtSV6a5It1HJcleUZHmaOS/CLJpiRn\nAfuOM4SbSynXllI2lFJ+AHwaWNqo915JPpfkt/W0L0ryvzqmnSRvSHJ53ZJ+ZZI3jTK/d0nyySSX\nJrlfPexBSc6t18XFSQ4f5XL7X9XLZxPwgvq75yS5pJ7u+iSvG2HZHd0x7IYkx3XU/awk36vn8WdJ\nHtUxzovrKwU3J/kCjR9Qo0nyiCRr6vm6AHgojeSns3U2yd5Jvprk+no6Fyd5apJ9gO/Vo91Qtzx+\nsh7n7CQfTLIqyXXAt0abb2Cg3oaHl/PjG7Fs1/0iyTFJtg5/D5wIHJI7Wz+Hl2HnvnVQkrPqbfH3\nST6WZLfG96cl+VKS1ye5qi7zb0l2GmN5vjLJL5PcmmQojZbJJOuBZwPHN5fPKP5cSrmu3u6HX3f8\nYErysnr73Fy/v7IjjnfW+9otSX6V5G3DsY+2nDJCl5EkC+phj68/D28PT03y0yRbgMfU3z0zyWAd\n0y+TvDXJqOfO+lj4lVLKL0op60spHwAuojpWSppJpRRfvib1omqBvBF4L7BLl3JbgaPrv3cHfg98\nClgMHAkMAbcDB9dlDqvH+S+qE8Ni4AfADxt1HgycAAxQtSKdBNwC3K9RZj3wmi5xPYEqYdsfeBDw\n78AGYLeO2H8N/BWwH/D+ep7vWX9/P2Az8K66nuV1HbcD9+gy7W1iA/4HcD5wamPYfYHXAQ+mSpz/\nlqpl6GGNMqfUy/NY4P7AI4GX1N/tU8d/MLALVav7T4F71d/fhaq16ZvAQcCj6xhub6yv4Tp+BRxT\nf14ILAP+DLwZeCBwXL38jxtpvTeG3TBcplH3WuCpdT1nAFcAd6nLPLKezuvr719N1fJ3fZdluxtw\nDfCZevs4Cvgl229jd6wj4OtUSeqSelkfRbXtBXhWXfYBwB7A7vU4ZwMbgXfW637/Ebb34Xn8db38\nhrezjcBf1GWO75wf4JnA7fXf84B3UyVKf1nHcLcRpjUf+F29DAeAw+v19slGvacBfwQ+RHX14Cjg\nZuClXZbns4BbgZfX62AFcBtwWP39vYFvAKvr+HYfpZ4TgTVdpvNC4Lf1vO9TL6/rgBc1yry53ib2\nBp4OXAX8fbflVNd1x7qvyy6ol93jO445F1K1NN8fuCfwuHp5HVvXc0S9TP9pAsfJI4CbgCf26ljt\ny1e/vnoegK+5/apPgL8HNgHnAu8AHtxRpnkifgXVZeJdGt+/lJETkMMbZZ5WD+uWTF8MvKrxuWuS\nO8L4d6FKPo7qiH1l4/P8ethT6s//AlzcUc/JjC/J3Vyf/DZxZ1I/6jj1eF8D3lX/ffe6jpeMUnb4\n5P4Y4DvA95sJCFVieSvwl41hRzBykvbqjrpPB77VMeyU5rJg/EnuixvfD9QxH1B//izwtY46VtM9\nyf2bEbaxl4+yjQ0nuT9nlMSls2xj+NnAT0coP9Ly+/vG9zsB/82dyVnXJLf+PGKC2DGtE6j2xXkd\n+82fh9cxVZJ7BZBGmc8Dn+uyPM8FPtIx7PPN9QJ8iUYyPUo9J9ax3Ei13d8EnN/4/nLg+R3j/CNw\nXpc6Xw9c0G05NdbBeJLc/9kx7neorlQ1h70Q+N0Y83qPev7+RLV/v7hbeV++fE3Py+4K2iGllC9R\ntTg+g6pF8DBgzfDl1BEcAFxUSvlTY9gFo5S9uPH3hvp9D4AkuyV5T31J84YkN1G1+O493tiT7JHk\n1FRdEP5IleDuNkIdd8RRStlEdZLeox60GPhxR/kfjTOEdwOHULXUPpGq1fAbSVLHd5ck/5Sqm8If\n6nl8SiO+AaoW2u9tX/Wds0mVFM4Hjiyl3NT47gDgN6WU6xrDRlsXgx2fB6hu3mo6D9h/OP4J6FzP\n4c7lO8DEl+9itt/GxhrnA8A/peq6sTLJg8coP6xzuYzm/OE/StXP86dU8zaVFgM/L6VsaQw7j+rH\n24Maw9aWUkrj8wbuXN4jGaD6AdZ0HpOLfx3VNj/8eg5AkvlULeWfSNX96KZ6e/9HqlZV6nLPr9fR\nhvr7f2YC+/wYCtuvz0OAt3bEdCqwMMm8LnXdVI/7sHoeVjW7qEiaGd54ph1WJxNn1a93JDmVqvvA\nZ3aw6tuak6nfh3+YvZeq1fH1VJcPNwNfoEr6xuszwF8Af0fVsnYrVTLSWcdtHZ8LU9Of/fflzht1\nfpXktfX0n0CVuL6hju21VDfs3EJ1Y9pwfJvHOZ3/pLrc+miq1sfJuGUS4xS2v+nqriOU67aeZ0Qp\n5RNJvkV1CfwpwJuSvK6U8qExRp3Mcum0lfEtp6kyXdvzePyplLJ+hOF3r99fxvY/tG4HSPXEltOB\nfwK+TfWjdDlVl55uttbvzWU82vLtXJ93B95K1dVnGx0/Jjq/K1Qt5gAXJVkCvAk4Z4xYJU0hW3I1\nHYaoWkRH8gvgwUmaJ5lHTGIajwY+VUr5aillLdXl6X0nUccHSilnllKGqE7+95lgHUNsH/+hIxUc\nh+EEb9dGfF8ppawupVxM1cWh+Ximy4EtVMl+tzo/QnWC/WpHa9IvgL2S/GVj2EjroowwbIj6xpyG\nxwKXNVoJrwMWDX+ZZH+qFuWx6u6cziM7ho21fIeAg5M0f6yMuU5KKb8rpfx7KeW5VD+iTqi/Gm4R\n7npz1hjuuJmuvlFqGXBpPeg6YPckuzbKb3MDZB3DWNMforrpqlnPY6mSxF9MJuhGvZ3r+jHcGf8O\nK9VTIq4CHlBKuaLj9eu62KHAlaWUd5ZS1pRSfsX2+/xIy2n4SsWixrBtbkTsYg3woBFiGuspEp3u\nQtU/WNIMsiVXk5bkXsB/AJ+kutnjJuDhwP8GvjzKaJ+j6rd7apJ3UvWXe339XfOkM9Il7+awy4Fn\nJxl+isPbRhmnm8uBFyUZpOqj9y6q/nMT8VHgdUneRfU4sIcx/keA7Z5kIVXce1P1ab2WOy8NXw48\np27B+iN3Pot0LUAp5dYkpwDvSnIb1SXkvwQOLKUM3+GeuuzwHfRfS3JUKeU8qv6GVwCfSfIGqn6E\n/0y1HsZaF+8FLkjyFqr+mY+mujHuFY0y3wNeneR8qmPNO7kzYexWd9MHgHOTvB74ClU/4iPHGOdz\n9Xx8PMnJVJe7Xz9CuTumnWQVVXeby4B7UbWmDydxv6ZaHs9I8g1gcylloi24f5vkl1QJ4+uobmo6\nrf7ux1Tb3clJPkCVEHduQ1cC909yCNXNWTd1dMeAqv/ySuDTSU6i6oLwAeAzHV1SJurdwOeT/Az4\nLnA0VV/8CT3NZBxOBP41yY1UNwHejWp/umcp5f1U+8PeSZ4P/AT4n1Q3pzVdyfbLaUu9Df5Dkiup\n9qG3jzD9kbbFt1HtM78B/i9Vq/AhwEGllH8aaSaS/ANVd5Rf1fPwdKorKa8YqbykadTrTsG+5u6L\n6rL5O6hOONdTJbmXUp1o79Yod8fd+vXnR1HdxbyZ6tLk8+syw3enb3ejD9WJ5XZg7/rzPlQn3Jup\nTmyvpEqq3tcY5wq6P13hEKoE4xaqvoLP7hynM/Z62PVs+xSBo6hayjZR3dx1fGf8I0x7fV1m+HU1\n1U1lzZtj/oLqMulGqn6TJ1ElRl/sqOtNddxb6nrf2FhGnXeVr6BKmB9Vfz6A6hLqZqrk+elUJ/In\nj1ZHo65nUfWnHZ7uio7vF1EljjfWy/fI5rIbJb4F9bDHN4a9mCrRvJnqx9MKutx4Vo/zCKpWuM1U\n/SyPofuNZx+gSnA31eviNOqnH9Tf/yNVS+OfqW+wour68b4Rpt35dIrbqbbx8+t4Lm7OX13u6Hob\nupkqmX8p2954tgvVUxOur+s7rnNa9ecDqfaLW6haMD8CzG98P9L2swr43hjL8+XceeVgCHhBx/fj\nvfFs1Kcr1GX+V2O9/b5exs9sfP9Oqh+CG6l+zLymuS10WU6LqW6gu7neHo5obmed20NHTE8GfliP\newNV/+5uT6N4e70ub6nn4Vzgud3m25cvX9PzSinjuWIjTZ8kLwQ+ASwopdza63j6WZLHUCW9Dywj\n952UJGlOsLuCZlySF1G1PP4OeAhV68znTXBnXpJjqFqoLqd61uv7gXNNcCVJc51JrnphT6q+bgup\nLsN/nupf3Grm7U7VF3gvqkur3wH+vqcRSZI0BeyuIEmSpNbxEWKSJElqHZNcSZIktY5JriRJklrH\nJFeSJEmtY5IrSZKk1jHJlSRJUuuY5EqSJKl1THIlSZLUOia5kiRJah2TXEmSJLWOSa4kSZJap2+S\n3CRvSnJBkhuTXJPkS0kOGMd4hycZTLIlyWVJjp+JeCVJkjR5fZPkAo8DPgg8EngScFfg20l2HW2E\nJPsCXwfOAg4B/hX4eJInT3ewkiRJmryUUnodQ08kuQ9wLfD4Usq5o5Q5BXhaKeXgxrDVwIJSylEz\nE6kkSZImqp9acjvdEyjA9V3KPAr4bsewM4FDpysoSZIk7bidex1ALyQJ8H7g3FLKpV2K7glc0zHs\nGuAeSe5WSrl1hLrvDRwJXAlsmZqIJUnqC/OAfYEzSyl/6HEsmuP6MskFPgwsAR4zDXUfCXx2GuqV\nJKlfvBD4XK+D0NzWd0lukn8DjgIeV0rZMEbxq4GFHcMWAjeO1IpbuxLg9NNPZ2BgYEdC1SyxYsUK\nVq1a1eswJI3CfbQ9hoaGOPbYY6E+l0o7oq+S3DrBfSZwWCnlv8cxyo+Ap3UMe0o9fDRbAAYGBli6\ndOmk4tTssmDBAtelNIu5j7aS3f20w/rmxrMkH6a6/PEC4JYkC+vXvEaZf0ny6cZoHwX2S3JKkgcl\neRXwXOB9Mxq8JEmSJqRvklzgFcA9gO8DVzVef9UoswjYa/hDKeVK4OlUz9X9GbACeGkppfOJC5Ik\nSZpF+qa7QillzIS+lPKSEYadAyyblqAkSZI0LfqpJVealOXLl/c6BElduI9KGolJrjQGT6DS7OY+\nKmkkJrmSJElqHZNcSZIktY5JriRJklrHJFeSJEmtY5IrSZKk1jHJlSRJUuuY5EqSJKl1THIlSZLU\nOia5kiRJah2TXEmSJLWOSa4kSZJaxyRXkiRJrWOSK0mSpNYxyZUkSVLrmORKkiSpdUxyJUmS1Dom\nuZIkSWodk1xJkiS1zs69DkCSpJFs2rSJdevWTUldixcvZv78+VNSl6S5wSRXkjQrrVu3jmXLlk1J\nXYODgyxdunRK6pI0N5jkSpJmpcWLFzM4ONi1zNAQHHssnH46DAx0r0tSfzHJlSTNSvPnzx936+vA\nANhQK6nJG88kSZLUOia5kiRJah2TXEmSJLWOSa4kac5atAhOPLF6l6SmvkpykzwuyVeT/C7J1iRH\nj1H+sLpc83V7kj1mKmZJ0ugWLYKVK01yJW2vr5JcYDfgZ8CrgDLOcQqwP7Bn/VpUSrl2esKTJEnS\nVOirR4iVUr4FfAsgSSYw6nWllBunJypJkiRNtX5ryZ2MAD9LclWSbyd5dK8DkiRJUncmud1tAF4O\nPAd4NvAb4PtJHtLTqCRJktRVX3VXmKhSymXAZY1B5yd5ALACOL7buCtWrGDBggXbDFu+fDnLly+f\n8jglSZprVq9ezerVq7cZtnHjxh5FozZKKeO9/6pdkmwFjimlfHWC470LeEwp5TGjfL8UGBwcHBz3\nv6OUJEmwZs0ali1bBrCslLKm1/FobrO7wsQ9hKobgySpxzZvhrVrq3dJauqr7gpJdgMeSHUzGcB+\nSQ4Bri+l/CbJycB9SynH1+VfC6wH1gLzgBOAJwBPnvHgJUnbGRqCZctgcBC8eCapqa+SXOBhwNlU\nz74twHvr4Z8G/prqObh7NcrvUpe5L7AJuAg4opRyzkwFLEmSpInrqyS3lPIDunTRKKW8pOPzu4F3\nT3dckiRJmlr2yZUkSVLrmORKkiSpdUxyJUmS1DomuZIkSWodk1xJkiS1Tl89XUGS1C4DA3DJJbDf\nfr2ORNJsY5IrSZqzdt0VDjyw11FImo3sriBJkqTWMcmVJElS65jkSpIkqXVMciVJktQ6JrmSJElq\nHZNcSZIktY5JriRpztqwAVaurN4lqckkV5I0Z23YACedZJIraXsmuZIkSWodk1xJkiS1jkmuJEmS\nWsckV5IkSa2zc68DkHpp06ZNrFu3bkrqWrx4MfPnz5+SuiRJ0o4xyVVfW7duHcuWLZuSugYHB1m6\ndOmU1CVJknaMSa762uLFixkcHBz1+6EhOPZYOP10GBgYuy5JM2vePFiypHqXpCaTXPW1+fPnj6v1\ndWAAbKSVZp8lS2Dt2l5HIWk28sYzqQtbiSRJmptsyZW6sJVIkqS5yZZcSZIktY5JriRJklqnr5Lc\nJI9L8tUkv0uyNcnR4xjn8CSDSbYkuSzJ8TMRqyRJkiavr5JcYDfgZ8CrgDJW4ST7Al8HzgIOAf4V\n+HiSJ09fiJIkSdpRfXXjWSnlW8C3AJJkHKO8EriilPKG+vMvkjwWWAF8Z3qilCRJ0o7qt5bciXoU\n8N2OYWcCh/YgFklSh0svhQMPrN4lqckkt7s9gWs6hl0D3CPJ3XoQjySpYcuWKsHdsqXXkUiabUxy\npS5sJZIkaW7qqz65k3A1sLBj2ELgxlLKrd1GXLFiBQsWLNhm2PLly1m+fPnURqhpZSuRJE2P1atX\ns3r16m2Gbdy4sUfRqI1Mcrv7EfC0jmFPqYd3tWrVKpYuXTotQUmSNNeN1PCzZs0ali1b1qOI1DZ9\n1V0hyW5JDknykHrQfvXnvervT07y6cYoH63LnJLkQUleBTwXeN8Mhy5JkqQJ6KskF3gYcCEwSPWc\n3PcCa4CT6u/3BPYaLlxKuRJ4OvAkqufrrgBeWkrpfOKCJEmSZpG+6q5QSvkBXRL7UspLRhh2DuC1\nE0mSpDmk31pyJUktsmgRnHhi9S5JTX3VkitJapdFi2Dlyl5HIWk2siVX6sJWIkmS5iZbcqUubCWS\nJGlusiVXkiRJrWOSK0mSpNYxyZUkSVLrmORKkiSpdUxyJUlz1ubNsHZt9S5JTSa5kqQ5a2gIDjqo\nepekJpNcqQtbiSRJmptMcqUubCWSJGluMsmVJElS65jkSpIkqXVMciVJktQ6JrmSJElqHZNcSZIk\ntc7OvQ5AkqTJGhiASy6B/fbrdSSSZhuTXEnSnLXrrnDggb2OQtJsZJIrdWErkSRJc5NJrtSFrUSS\nJM1N3ngmSZKk1jHJlSRJUuuY5EqSJKl1THIlSZLUOia5kqQ5a8MGWLmyepekJpNcSdKctWEDnHSS\nSa6k7ZnkSl3YSiRJ0tzUd0lukr9Nsj7J5iTnJ3l4l7KHJdna8bo9yR4zGbN6x1YiSZLmpr5KcpM8\nH3gvcCLwUODnwJlJ7tNltALsD+xZvxaVUq6d7lglSZI0eX2V5AIrgI+VUj5TSlkHvALYBPz1GONd\nV0q5dvg17VFKkiRph/RNkpvkrsAy4KzhYaWUAnwXOLTbqMDPklyV5NtJHj29kUqSJGlH9U2SC9wH\n2Am4pmP4NVTdEEayAXg58Bzg2cBvgO8nech0BSlJkqQdt3OvA5jNSimXAZc1Bp2f5AFU3R6O7zbu\nihUrWLBgwTbDli9fzvLly6c8TknqV/PmwZIl1bvmltWrV7N69epthm3cuLFH0aiN+inJ/T1wO7Cw\nY/hC4OoJ1HMB8JixCq1atYqlS5dOoFpJ0kQtWQJr1/Y6Ck3GSA0/a9asYdmyZT2KSG3TN90VSim3\nAYPAEcPDkqT+/F8TqOohVN0Y1AdsJZIkaW7qp5ZcgPcBn0oySNUiuwKYD3wKIMnJwH1LKcfXn18L\nrAfWAvOAE4AnAE+e8cjVE7YSSZI0N/VVkltKOaN+Ju7bqLop/Aw4spRyXV1kT2Cvxii7UD1X975U\njxq7CDiilHLOzEUtSZKkieqrJBeglPJh4MOjfPeSjs/vBt49E3FJkiRp6vRNn1xJkiT1D5NcSZIk\ntY5JriRJklrHJFeSNGddeikceGD1LklNJrmSpDlry5Yqwd2ypdeRSJptTHKlLmwlkiRpbjLJlbqw\nlUiSpLnJJFeSJEmtY5IrSZKk1jHJlSRJUuv03b/1lSTNDpdfDjfdtGN1DA1t+z5Zu+8O+++/Y3VI\nml1MciVJM+7yy+GAA6auvmOP3fE6LrvMRFdqE5NctdqOthRNVSsR2FIkNQ3vl6efDgMDvY1laKhK\nkne0VVnS7GKSq9aaypaiqWglAluKpE4DA7B0aa+jkNRGJrlqLVuKJEnqXya5aj1biiRJ6j8+QkyS\nJEmtY5IrSZKk1jHJlSRJUuuY5EqSJKl1THIlSZLUOia5kiRJah2TXEmSJLWOSa4kSZJaxyRXkiRJ\nrWOSK0mSpNYxyZUkSVLrmORKkiSpdfouyU3yt0nWJ9mc5PwkDx+j/OFJBpNsSXJZkuNnKlZJkiRN\nTl8luUmeD7wXOBF4KPBz4Mwk9xml/L7A14GzgEOAfwU+nuTJMxGvJEmSJqevklxgBfCxUspnSinr\ngFcAm4C/HqX8K4ErSilvKKX8opTyIeD/1vVIkiRpluqbJDfJXYFlVK2yAJRSCvBd4NBRRntU/X3T\nmV3KS5IkaRbomyQXuA+wE3BNx/BrgD1HGWfPUcrfI8ndpjY8SZIkTZWdex1AW61YsYIFCxZsM2z5\n8uUsX768RxFJkjR7rF69mtWrV28zbOPGjT2KRm3UT0nu74HbgYUdwxcCV48yztWjlL+xlHJrt4mt\nWrWKpUuXTiZOSZJab6SGnzVr1rBs2bIeRaS26Zskt5RyW5JB4AjgqwBJUn/+wCij/Qh4Wsewp9TD\nNctl8yYeyjp2Hep1JLDrUPU4j2xeDMzvdTiSJLVe3yS5tfcBn6qT3QuonpIwH/gUQJKTgfuWUoaf\nhftR4G+TnAJ8kiohfi5w1AzHrUmYd+U61rAMju11JDAArAGGrhyEx9jCL0nSdOurJLeUckb9TNy3\nUXU7+BlwZCnlurrInsBejfJXJnk6sAp4DfBb4KWllM4nLmgW2rLvYpYyyGdPh4GB3sYyNAQvPBY+\nse/i3gYiSVKf6KskF6CU8mHgw6N895IRhp1D9egxzTFl1/lcyFI2DwA9bjzdDFwIlF17G4ckSf2i\nnx4hJkmSpD5hkitJkqTWMcmVJElS65jkSpIkqXVMciVJktQ6JrmSJElqHZNcSZIktY5JriRJklrH\nJFeSJEmtY5IrSZKk1jHJlSRJUuuY5EqSJKl1THIlSZLUOia5kiRJah2TXEmSJLXOzr0OQJLUf7J5\nEw9lHbsO9ToS2HUIHgpk82Jgfq/DkTRFTHIlSTNu3pXrWMMyOLbXkcAAsAYYunIQHrO01+FImiIm\nuZKkGbdl38UsZZDPng4DA72NZWgIXngsfGLfxb0NRNKUMsmVJM24sut8LmQpmweAHjeebgYuBMqu\nvY1D0tQyyVVrbdpUva9Z09s4oGopkiRJM8ckV621bl31fsIJvY2jaffdex2BJEn9wSRXrXXMMdX7\n4sUwf5I3TA8NwbHHwulT0G9w991h//13rA5JkjQ+JrlqrfvcB172sqmpa2AAlnrTtSRJc4b/DEKS\nJEmtY5IrSZKk1jHJlSRJUuuY5EqSJKl1THIlSZLUOn2T5Cb5iySfTbIxyQ1JPp5ktzHGOS3J1o7X\nN2YqZvXevHmwZEn1LkmS5o5+eoTY54CFwBHALsCngI8Bx44x3jeBFwOpP986PeFpNlqyBNau7XUU\nkiRpovoiyU2yGDgSWFZKubAe9nfAfyb5+1LK1V1Gv7WUct1MxClJkqSp0S/dFQ4FbhhOcGvfBQrw\nyDHGPTzJNUnWJflwkntNW5SSJEmaEn3RkgvsCVzbHFBKuT3J9fV3o/km8AVgPfAA4GTgG0kOLaWU\n6QpWkiRJO2ZOJ7lJTgbe2KVIAQYmW38p5YzGx7VJLgZ+BRwOnN1t3BUrVrBgwYJthi1fvpzly5dP\nNhxJklpj9erVrF69epthGzdu7FE0aqM5neQC7wFOG6PMFcDVwB7NgUl2Au5VfzcupZT1SX4PPJAx\nktxVq1axdOnS8VYtSVJfGanhZ82aNSxbtqxHEalt5nSSW0r5A/CHscol+RFwzyQPbfTLPYLqiQk/\nHu/0ktwPuDewYRLhSpIkaYb0xY1npZR1wJnAqUkenuQxwAeB1c0nK9Q3lz2z/nu3JO9K8sgk+yQ5\nAvgycFl+pXiUAAAgAElEQVRdlyRJkmapvkhyay8A1lE9VeHrwDnAyzvK7A8Md6S9HTgY+ArwC+BU\n4CfA40spt81EwOq9Sy+FAw+s3iVJ0twxp7srTEQp5Y+M8Y8fSik7Nf7eAjx1uuPS7LZlS5XgbtnS\n60gkSdJE9FNLriRJkvqESa4kSZJaxyRXkiRJrdM3fXIlSbPHpk3V+5o1vY0DYGio1xFImg4muZKk\nGbduXfV+wgm9jaNp9917HYGkqWSSK0maccccU70vXgzz50++nqEhOPZYOP10GJj0P3GvEtz995/8\n+JJmH5NcqYtFi+DEE6t3SVPnPveBl71s6uobGAD/k7qkJpNcqYtFi2Dlyl5HIUmSJsqnK0iSJKl1\nTHIlSZLUOia5kiRJah2TXEmSJLWOSa4kac6aNw+WLKneJanJpytIkuasJUtg7dpeRyFpNrIlV+pi\n8+bqBLp5c68jkSRJE2GSK3UxNAQHHeT/tpckaa4xyZUkSVLrmORKkiSpdUxyJUmS1DomuZIkSWod\nk1xJkiS1jkmuJGnOuvRSOPDA6l2SmkxyJUlz1pYtVYK7ZUuvI5E02/gfz6QuBgbgkktgv/16HYkk\nSZoIk1ypi113rS6FSpKkucXuCpIkSWodk1xJkiS1Tt8kuUnenOS8JLckuX4C470tyVVJNiX5TpIH\nTmeckiRJ2nF9k+QCdwXOAD4y3hGSvBF4NfA3wCOAW4Azk+wyLRFKkiRpSvTNjWellJMAkhw/gdFe\nC7y9lPL1etzjgGuAY6gSZklSDy1aBCeeWL1LUlM/teROSJL7A3sCZw0PK6XcCPwYOLRXcUmS7rRo\nEaxcaZIraXsmuaPbEyhULbdN19TfqQ9s2FCdQDds6HUkkiRpIuZ0kpvk5CRbu7xuT3JAr+PU3LVh\nA5x0kkmuJElzzVzvk/se4LQxylwxybqvBgIsZNvW3IXAhWONvGLFChYsWLDNsOXLl7N8+fJJhiNJ\nUnusXr2a1atXbzNs48aNPYpGbTSnk9xSyh+AP0xT3euTXA0cAVwEkOQewCOBD401/qpVq1i6dOl0\nhCZJ0pw3UsPPmjVrWLZsWY8iUtvM6e4KE5FkrySHAPsAOyU5pH7t1iizLskzG6O9H3hLkmckeTDw\nGeC3wFdmNHhJkiRNyJxuyZ2gtwHHNT6vqd+fAJxT/70/cEcfg1LKu5LMBz4G3BP4IfC0Usqfpj9c\nSZIkTVbfJLmllJcALxmjzE4jDFsJrJyeqCRJO2LzZrjiCthvP9h1115HI2k26ZvuCpKk9hkagoMO\nqt4lqckkV+pi3jxYsqR6lyRJc0ffdFeQJmPJEli7ttdRSJKkibIlV5IkSa1jkitJkqTWMcmVJElS\n65jkSpIkqXVMciVJktQ6Pl1BkjRnDQzAJZdU/wxCkppMciVJc9auu8KBB/Y6Ckmzkd0VpC4uvbQ6\ngV56aa8jkSRJE2GSK3WxZUuV4G7Z0utIJEnSRNhdQX1t06ZNrFu3btTvh4a2fe9m8eLFzJ8/f4oi\nkyRJO8IkV31t3bp1LFu2bMxyxx47dl2Dg4MsXbp0CqKSJEk7yiRXfW3x4sUMDg5OWV2SJGl2MMlV\nX5s/f76tr5IktZA3nkmS5qwNG2DlyupdkppMciVJc9aGDXDSSSa5krZnkitJkqTWMcmVJElS65jk\nSpIkqXVMciVJktQ6JrmSJElqHZNcSZIktY5JriRpzpo3D5Ysqd4lqcn/eCZJmrOWLIG1a3sdhaTZ\nyJZcSZIktY5JriRJklqnb5LcJG9Ocl6SW5JcP85xTkuyteP1jemOVZIkSTumn/rk3hU4A/gR8NcT\nGO+bwIuB1J9vndqwJEmSNNX6JsktpZwEkOT4CY56aynlumkISZIkSdOkb7or7IDDk1yTZF2SDye5\nV68DkiRJUnd905I7Sd8EvgCsBx4AnAx8I8mhpZTS08gkSZI0qjmd5CY5GXhjlyIFGCilXDaZ+ksp\nZzQ+rk1yMfAr4HDg7G7jrlixggULFmwzbPny5SxfvnwyoUiSRnDppfC858F//Ef1zFzNHatXr2b1\n6tXbDNu4cWOPolEbZS43SCa5N3DvMYpdUUr5c2Oc44FVpZRJdTtIci3wj6WUU0f5fikwODg4yNKl\nSyczCUnSOK1ZA8uWweAgeMid+9asWcOyZcsAlpVS1vQ6Hs1tc7olt5TyB+APMzW9JPejSqo3zNQ0\nJUmSNHF9c+NZkr2SHALsA+yU5JD6tVujzLokz6z/3i3Ju5I8Msk+SY4AvgxcBpzZk5mQJEnSuMzp\nltwJehtwXOPz8GWQJwDn1H/vDwx3pL0dOLge557AVVTJ7VtLKbdNe7SSJEmatL5JckspLwFeMkaZ\nnRp/bwGeOt1xSZIkaer1TXcFSZIk9Q+TXEmSJLWOSa4kac5atAhOPLF6l6SmvumTK0lqn0WLYOXK\nXkchaTayJVeSJEmtY5IrSZKk1jHJlSRJUuuY5EqSJKl1THIlSZLUOia5kiRJah2TXEnSnLV5M6xd\nW71LUpNJriRpzhoagoMOqt4lqckkV5IkSa1jkitJkqTWMcmVJElS65jkSpIkqXVMciVJktQ6JrmS\nJElqHZNcSZIktc7OvQ5AkqSRbNq0iXXr1nUts2ULnHFG9b5mzejlFi9ezPz586c4QkmzmUmuJGlW\nWrduHcuWLZuSugYHB1m6dOmU1CVpbjDJlSTNSosXL2ZwcHDK6pLUX0xyJUmz0vz58219lTRp3ngm\nSZKk1jHJlSRJUuuY5EqSJKl1THIlSZLUOia50hhWr17d6xAkdeE+KmkkfZHkJtknyceTXJFkU5LL\nk6xMctdxjPu2JFfV430nyQNnImbNHp5ApdnNfVTSSPoiyQUWAwFOAJYAK4BXAO/oNlKSNwKvBv4G\neARwC3Bmkl2mNVpJkiTtkL54Tm4p5UzgzMagK5O8hyrRfUOXUV8LvL2U8nWAJMcB1wDHAGdMU7iS\nJEnaQf3SkjuSewLXj/ZlkvsDewJnDQ8rpdwI/Bg4dNqjkyRJ0qT1RUtup7pf7auB13UptidQqFpu\nm66pvxvNPIChoaEdCVGzyMaNG1mzZk2vw5A0CvfR9micO+f1Mg61Q0opvY5h0pKcDLyxS5ECDJRS\nLmuM8z+A7wPfK6W8vEvdhwLnAvctpVzTGP55YGspZfko470A+OxE5kOSJG3jhaWUz/U6CM1tc70l\n9z3AaWOUuWL4jyT3Bb4HnNstwa1dTXWz2kK2bc1dCFzYZbwzgRcCVwJbxpiGJEm60zxgX7a9j0aa\nlDndkjsRdQvu94CfAC8q45jxJFcB7y6lrKo/34Mq4T2ulPIf0xmvJEmSJq8vbjyrW3C/D/ya6mkK\neyRZmGRhR7l1SZ7ZGPR+4C1JnpHkwcBngN8CX5mZyCVJkjQZc727wng9Gdivfv2mHhaqPrs7Ncrt\nDywY/lBKeVeS+cDHqJ7G8EPgaaWUP81E0JIkSZqcvumuIEmSpP7RF90VJEmS1F9MctWXkmxNcnQP\nprs+yWtmerrSbNbG/SLJPvVx5uBexyL1K5NctVKS+yT5SJJfJ9mSZEOSb9bPP4bqH3p8s5cxSnNB\nktPqZG349ft6X3pwr2ObA+wPKPWQSa7a6ovAIcCLqG4ofAbVEzbuDVBKubaUclvPopPmlm9SPSN8\nT+CJwJ+Br/U0ojEkuWuvY6C6wVlSj5jkqnWSLAAeC7yxlHJOKeU3pZSfllJOKaV8vS6zTXeFJI9O\ncmGSzUnOrx8bd8elxiSH1Z+fmOQnSW5Jcl6SAxp17Jfky0muTnJTkguSHDHT8y9Ng1tLKdfVPw4v\nAt4J7JXk3gBJ3pnkF/V+8askb0vSfHIN9T51Qb2PXZfkC6NNLMnLktyQ5An157sn+WySm5P8Jsnf\nJTk7yfsa46xP8pYkn06ykeqpOCR5cJKzkmyqW6E/lmS3xnjb1FMP+1KST3bU/aYkn0hyY32F6ISO\ncR6RZE09fxcAD8WWXKmnTHLVRjfXr2OS7DJW4SS7A18Ffk51YjoReBcjn6D+GVgBLKNqzfpE47u7\nA/8JPAF4CFXr11eT3G/ScyLNMknuTnWF5PJSyh/qwTcCxwEDwGuAl1HtJ8PjPJ3q6srXqfaNw4Hz\nR6n/DcC/AE8qpZxdD14FHAr8T+DIevyHjjD664Gf1dN4e/0IyG8Bf6DaZ58LPAn44IRnHF5H9c+E\nHgJ8GPhIkv3rmHejatm+BFgKrKT6j5ySeqhfnpOrPlJKuT3J8cCpwCuTrAF+APyfUsrFI4zyQmAr\n8Df1M5DXJXkP8O+dVQNvLqWcC1XrFfD1JLuUUv5Ut3Bd1Ch/YpJnA0dTnRSlueoZSW6q/94NuIoq\n4QSglPIvjbL/neS9wPO5M9F7M/C5UsrbGuXWdk4kySlU++PjSynr6mF3p0qg/1cp5fv1sJfUMXQ6\na/g/VNblTgDuRvVfKrcAQ0leDXwtyRtLKdeNdwEA/1lK+Wj99ylJVlD9oL28jjnAy+pjyFCSvXC/\nl3rKlly1UinlS8B9qfrifhM4DFiT5LgRih8AXNTxTz4uGKXqZpK8oX7fA6rWnCTvSXJpfan1JmAx\nsPcOzIo0G3wPOJiqn/vDgTOBb9WJHEmen+Tc+gbPm6iueDS3+4fUdXTz98BLgccOJ7i1/agaZH4y\nPKCUciPwixHqGOz4vBj4eZ3gDjuP6tz3oDHi6dT5A/lq6n2/nk7nMeRHE6xf0hQzyVVr1a2rZ5VS\n3lFKeSzwKeCkHay2ebPacHeG4f3ovcAzgX+g6hN8CNXlyzG7TEiz3C2llPWllCtKKYPACVQtuick\neRRwOlVXhKdTJbTvYNvtfvM4pnEO1X+gfP6OxDmJcbay/Q1iI9201nmjasFzqDSruYOqnwxRnZg7\n/QJ4cMfd2I+YRP2PBj5VSvlqKWUtcC2w7yTqkeaCAuxKtd1fWUp5ZyllTSnlV2y/3V8EjHUT5gXA\n04A3J3l9Y/gVVP3fHz48oL659ADGNgQckmTXxrDHArdzZ0vwdcCiRt13AQ4aR92d0zm44x6AQ0cr\nLGlmmOSqdZLcq76b+oX1ndX7Jnke8L+BL48wyueoWpBOTbI4yZFUN7DAtjefjfQ4oOawy4FnJzkk\nySHAZ0cZR5pr7pZkYf1aTHXj1nyqm60uB/auuyzsl+qfOhzTMf5JwPIkK+t97MH1DWbbKKWcDxwF\nvDXJa+thNwOfBt6T5PAkBwIfp0pUx3p6wWeBLcCnkxxYP63hA8BnGv1xvwc8PclRSR4EfAS458QW\nD5+rY/l4koEkR3HnMURSj5jkqo1uprpz+/+juuHsYqqT7MeAv6vL3HFyLKXcRHUTzSHAhcDbubNb\nQ7Mv30gn1Oaw1wE3UPX5+wrVXd1rupSX5oqnUt3odRXVvrUMeG79iL6vUT394INU+8+jgOYNZpRS\nfgA8j6qP/IXAd2m0zLLt/nge1f749iR/Ww9+HfBfVEn1t4FzgXWMsX+WUjZTPY3hXlQtxWcA3+HO\n4wDAJ6mS6E9TPUv7V2zff7jrvl9KuaWet4Oo9vm3A9sl8ZJmVkrxnCt1SvJCqseDLSil3NrreCTd\nqX402O+A15VSTut1PJJmJx8hJgFJXkTV9+93VDfOvBP4vAmu1HtJHkL1BIMLqLoSvJWqJfUrvYxL\n0uxmkitV9qS6xLqQ6tFgnwfe0tOIJDX9PdXNZn+ielTYY0sp1/c2JEmzmd0VJEmS1DreeCZJkqTW\nMcmVJElS65jkSpIkqXVMciVJktQ6JrmSJElqHZNcSZIktY5JriRJklrHJFeSJEmtY5IrSZKk1jHJ\nlSRJUuuY5EqSJKl1THIlSZLUOia5kiRJah2TXEmSJLWOSa4kSZJaxyRXkiRJrWOSK0mSpNYxyZUk\nSVLr7HCSm+Tfk/whye1JDp7AeOuTvGZHpz9RSY5PcsM01LtPkq0TWQYTqPu0JF/cwToOq9fRPerP\n07IcRpn28Umun4lpzSX1Otk6vE5maJonJlkzU9NrTHfC29t07lPaVufxYZQyM3bM6KV6mzt6guMc\nk+TyJLcled90xTbVenU8aEz7wl5MeyTTFc9M5DpJzp5L2914TWZf7DShJLcz2UryVOA44ChgEXDJ\nCOPMxgNjmUjhcS7o/wb2ZIRlMAVeA7x4B+s4D1hUSrmxMWxCy2EH/B/ggOEPs+3gNhO6HIRmah0M\nezdwxAxPc9hk5nWml8+UmyPb+0jHh5HM+fUxDnsC35zgOB8FzgDuB/zTlEc0BUY5j83I8aDLObQn\n29Nsi0fTZ+cdHP+BwIZSyo+7lAl9sOGUUgpw7TTVfdMU1PFnpji+JHctpdw2jmnfCtzaOXgqY9H4\nlFI2AZt6HccEpNcBTJFp2d7Huw+OZTqOD720I8ullDKh5ZDk7sAewLdLKddMZpp1PVOyLidiDh4P\nNEN6sT1Oh0l3V0hyGvABYO/6V9EVI5Q5DPgksKAuc3uStzaK7JbkE0luTPLrJCd0jH+/JJ9PckPd\nJeLLSfbpEtPw5d+jkvw8yeYkP0py4Bjz8sokv0xya5KhJMc2vltPdYL68mjzWZfb5tJqknsm+WyS\na5NsSvKLJMd3ieG5SS6qy/4+ybeT7Fp/19mCfnaSDyRZleT6JFcneWmS+Uk+WS/Py+uW9s5lM+Ll\nyCT71cv36iQ3JbkgyREdZdYneUuSTyfZCHys23JtjHdHa369DE4EDmlsE8eNMt5pSb6U5E11XDfU\n098pybvqbeI3SV7cMV7X7SbJw+rle12SPyb5fpKHdtSxtV6mX0xyS5LLkjyj8f2412+9rxwGvLYx\nz3s3ijwsyU/q6ZyXZP+O8Z+ZZLDenn+Z5K1JRt13kxye5MdJbq6XwQ+T7FV/t02rYr0sP1CXuzbJ\nO5J8KsmXGmXOTvKvSU6pl+eGJCd2THNFvf3enOS/k3woyW6jxThK3I9IsqaezwuAh9KRHNbb8Y+T\nbElyVZKTm8silTcluaJeLxcmeU7j+4nul+OZ972SfKXebzbW294e9XeT2d7fWse3MclHkuzcKHN2\nkg+m2vevA741jhj2r6d9QMf0ViS5vP778HQcH5K8ONVx+eYkXwDuPULME902J7tPH5TkrNx5fPxY\nc/tq1PvmJL8D1tXDd0nyniS/refjR6nOS6NKo5Uvdx7Xn5Xke6n20Z8leVT9/WHAjVTb6dn1+n18\n/d1zklxSb6vrk7yuYzrbHU8b03teknPq+b2gXocPT3WcuCnJN5Lcu1FX12NaRjmPJVmZbY8Hqdfh\nb+q4L0xyZOP7rstjlOXZ9Rya5Nh6WfwxyeqO9XpkquPXDfV6/1qS/XoYT9Ll+NLFPZJ8rt4Gf5vk\nVR3THOn4Ob+jzGNS7f+3pDrvfzPJglHm8el1/Mvrz+M9zk/o2FJ/v113yrqOszvqHus4+sB6m9+c\nar950jiW69hKKeN+AacBX6z/3h14C/Br4C+Be49QfmeqS+031GX2AObX360HrgNeAewHvBH4M7B/\nY9y1wL8DS4AHAf8/MATsPEp8hwFbqboMPBE4EPgq8Ctgp7rM8cD1jXGeRdXK+HKqlukVwG3AYfX3\n96nrfFEd/3bzWZfbB7gdOLj+/G/AINWJeu86nqePMu6ewJ/qZbV3HfcrGsvqjuVefz4b+CPwZuAB\n9fttwH8CL62HfYiqZWZeY9ncDtxjlOVwMHACMFCPfxJwC3C/Rpn19bpcAdwfuP84t5s7pgXMo7pE\ndlFjm7hbl+1tI9WPqf2pumxspbqU+A91nP9Yr7/7jne7AZ4AvKCu80F12Q3Abo1pb6Xatv+Kavt8\nP9XJ7J6TWL/3oLoc/NHGPIc7t9f/Ah4LLAZ+APywMe7j6nV9LNU2dgTV9vxPo0xrp3odvRPYt56/\nFw2vR6qEa02j/D9S7YdHU3Up+XA9vc7t7Qaqy7APqOu7HTiiUeY19fzsDRwOXAr820jbwChx7wZc\nA3yGahs8Cvgl2+5T9wVurreHA+qYrwXe2jE/a4En1fN/HFVL1eMmut7GM+/1erywXm8PAR4O/AT4\n3iS39xuBz9XL4Gn1Mnl7Rzwb6/W7f/0aLYazG+P9GDipY3o/AVaOcnx4JNXx+PVUx8VXA9ez7TFj\nQtvmDuzT84HfUXUHGKDavn4FfHKEZfepusxAPfxU4IfAo6mOWa+rt4cHdIlxK3B047i+td6mnlov\nizOAK6gaiXau52Mr8Mx6/e4MLKuX35vrcY6jOp4e1+142jG9J1Htv/9Vr6uzgEcBhwCXAR9q1NX1\nmMYo5zG2Px6sqGN6Xl3XO+t18YDxLI9Rlme3ad8I/Ee9zh4DXMW22/uzgWPqZXMw8GXg5x3n3ZmM\np+vxZZTprafaT/43d+5LtzGx4+dDgM3AB4EH1+v4FcC9GseF99V/v6Ce3tMmcZwf77Hlex373hc7\n5nlVR5nxHEcvBr4NHER1Physyxw92rIdz2tihbdPtl4LXDHGOCOe3OoV/6mOYVcDf1P/fSxwacf3\nu1AdKJ40yrSGk4bnNob9RT3Oc0eKBzgX+EhHPZ8HvjbSQa/LfA7vbMMn5K8AHx/ncn1ovTL3Gudy\nPxv4QePzXYCbmssTWFjH84jGshk1yR1luhcDr+pYZ/93whvZ9st8mwPrGNvbFR3DhoDvjzDvf7UD\n281dqHbuozrW+crG5/n1sKdMdP021tn7RthebwcObwx7Wj1sl/rzd4A3doz3QuB3o0znL+rxRzzo\ndi57qhPhio5lcWW37a0e9mPgX7rM73OAa0fbBkYo/zdUCesujWEvZ9sk9x0jrNtXAhsb6/lm4JEd\nZU4FTt+B9TbqvANPpvqBet/G9wP1trJsEtv7dTSS4HoZbOyI56cd440nhtcClzW+P6BetsONCp3H\nh8/SOAbWw1az7X48oW2zMY8T3adPAH5P/YO9sZ/8GfjLRr1X0WgAAfaiSib27Jjed4B/7hLjSEnu\nizuW7e3AAfXnBXWZxzfKnA58q6PeU4CLG5+3O56OMr3n19M7rDHsjXTsCx31jHZMO7qjXOfx4Lcj\nrNMfAx8c7/IYa5l2TPsm6sacxjL6ry71DCeoS2Y6HsZxfBllWuuB/xxhX/p6l3E6j5+fBc7pUv5s\n4H3Aq6h+jD624/vxHucnc2w5jfElud2Oo0+h+jG1sPH9kSOtp4m+ev0IsYs7Pl9N9csKql9t+9dN\n5DcluQn4A3A3ql8CoynA+Xd8KOUG4BdUK2YkA1S/lJvO61J+vD4CLK8vZ5yS5NAuZX9O9Sv9kiRn\nJHlZknuOUf9Fw3+UUrZSLZuLG8OG+4btwTgk2S3VZb1L60saN1G1LO7dUXRwPPVNobUdn69h2/kc\nnvdxbzdJ9khyaqouCH+kOhnsxvbz2pzOJqpf+cPTmcj6HUtzP9hQvw9P5xDgrR3zcyqwMMm8zorq\n7f3TwLeTfDXJa5LsOdJEU12aXkj1y3x4/K2MvI4v6vi8oREjSZ6U5Lv1pbgbqVrP7z1SjKNYDFxU\nSvlTY9iP2LZP7uJ6WNN5wN2T3I+qlWQ+8J2O5fUiqtZ4mNx66zbvi4HflFKuGv6ylDJE1UoymWPI\nz0vVh33Yj6jmb6/GsM71M54Y/g9w/ySPqD+/kCq5uXyUOAaoTkJNnct+Qttmw0T36cVUy2VLY5zz\nqE7UD2oMu7hUfYuHPZjqysZlHTE+nu7nkJF07qOh+7F1oI6x6TyqY1Nzmx7teNqc3vCx/JKOYc39\nb7zHtFEl2Z3qasl4zocTXR6jubI+tjbras7XA+vL/L9K1aVjPdU5ftRj9TTG0+34Mtb21Lnv/IjG\nMh3H8fMhVDlCN8+jSnSfXEo5t1H3RI7zkzm2jNd4jqPNPu2dy2xSdvTGsx3V2am5cGc/4bsDP6Vq\neu+8+eS6aY5rh5VSvpWq3+VRVL+GvpvkQ6WUN4xQdivwlPqE+xTg74B3JHlEKeXXo0xipGU3Uifx\n8f6QeS/V5cbXU10K3Ax8gerXa9Mt46xvqoxnPie63XyGqsXz76ieinEr1Q+jznkddToTWb/j0JxO\nqd+b8/NWYLtHyHWc9JvD/zr5f+2df6ieZRnHP1/BLBBKJQZFdUagpeVKXVM3/8hsleSMJc4FW9IP\nkbSIbZUNq+mGJVvI2aCiLNjmBCMls6hkbf0gxg60SmkHc9ZBS6oRha3ESV798X3e9ew9z/s+z/Oe\nc3Y8764PjHGec/+4nvvHdd/PfV/XdTSKr+9WAJskXRERYwPIViVjR85TwHZxwEPYRGY9Pkm4DLgb\nt2mlnDPA6cX/V+JTvTLPwcD91m+8zQat52BE/FXSHjwvxoCVuL+mQuuxWdB2Tjelu11Ox6e9F+AT\noTJHWpbdb45OhV59WVVf97Ny/U112nQxXe1R1+/fxxvbj+A5fQr+SOqnq2dKnlr9MgiSRqjXn882\nKOoAHusfZvDDqEHW9xeYvNaeWpFuVvToiVDUR/HXdFsOYJuQwxHxh65//aINCNst+QfpDHw1d7BH\n+nFse1NmcVf652n2DnHcDxF/j4idEbEa2zrd0DdzxL6IuA2bLxzF9sInikuxucP3IuJ3+Op4ZIbq\nGnRMNKHJuLkU2BoRPy6+Sp/H12CtaNm/U5kH51S8S6UDZEm230bEnRGxGJ8AfaAizTP4RGhh55ns\nNHRBSxkvBBQR6yJiLCIOAa9uWcY4cL6k8uJ1CcfPqfHiWZklwL8i4k94zj4HvK6ivf7cydB2XjaQ\n+zWSjr2vpHOBV/D/E8s2fb9A0mmlny8BjkTEUwPKUNZju4AVslPOfGyW1a/MRV3Putt+oLE5AOO4\nXV5WerYEX0k/1iffr3G7z6uQsU0EhahPMomqdWUJNhmpK2+Q+protL7rWKEfn6Z+PRxEvqZr6DEk\nnYnX7k0RsTciHqPC+fFEyUND/dKDbke4i/EYAevbOv35CPWh3p7AttlXS9rWeThFPd9Evx3GIWTL\nvKVB2VX1zCs969b/A3EiNrkT+LrtcklndSmqfuzCdlgPSloiaUT2AB6V9KqavJ8v6nsTdkQ4jG3x\nqiO4XXMAAATuSURBVNgMXC/pxuJqZA3eXG7ueod3SJpXY0Zw7GtG0m2Slkl6vRzd4b302GjLXuWf\nlXRhcS35fqygem3MB6VfOKbHgeWSFkhagNt/psI3TeCr0wXFmJjO04Ym4+ZxYJWkN0hahO3nWoXR\nadO/BRPAItkb+KzSlWVVG5ef3Q6slj2ezy1kXiFpYw+5RiTdIeliSa+VtBRv+nvJtg1YX7zL2cAo\nVmBtlMsh4FTZNGK+pFXYlrQN9xZ13i3pjZKuxLcKZb6CFeE2SedIuhrYgG8hiIgjwBbgLkmr5Ygh\nb5V0cyHTIP3Wl4jYjT8idhV1vQ2bi+yNiI7X+gTNx/tLgG+W2mAD7qNBZSgH+n8AO0F+tfjdX7qK\nKo+7rcC7Ja0t9OLN2EauTKuxOQV24dOs7ZLOk/T2Qr4dEdHzVq8wxbgX2CF7348UuvYWSe9pUf8g\nevDLeM24VY6M8EHgJo5fV9rUVydDE502Qf06thn4jKRrJZ0t6UvYLGW0hSxVNKm7m39gs5Ubivl6\nOW7Xbt10QuRpol/6sFjSumIs3ARcg52ZoZn+/CKwUI668Oain2+UPwTKMh7CG93lku4q/WogPd9Q\nv+3BEYJWFbpiA3Yea8NuPIZ3SDpf0mXApu5EcoSVj03K3YcZ3+RGxD7sVX4fPh38VOdXVclL+Z7F\ntlNP4mvzg9je6zRsG9mzSuylO4ptUF4JXNVlq1WW70HslLEWd+ZHsRH7L0rJ1uKrzSfx6UW/ujsc\nBe7A9rY/xddmK3vkewa/6w/wycTtwJqIeLhBPW2e9RvQa7BS+SX+IPgRk9+1Mr8cruZbfcru5v6i\n/L14TFzXIu90jJsP4au9X+EJO8rkGKG96uk8b9O/YOX430Kev2GnmCbv8zDeiL0TXzPvAz6JlXQV\n/8H2Td/BY+lr2Gnk6z3S34k3AtuxLd4R7OFavm6uU4SP4PHzaWwbtxLPwcZExL+Bq7ByPABsLMor\np3kaXxUuBH6DN73fwA5pnTSfK/Legtv6h0WePxZJ2vZbk83+Mjx3fobb7hDHj+k24/0nWNn/HDun\nfBdHOqmTp06GziL9ELZbv6eijPK424914SdwW1+B27VcXtux2Ysmc/pdwJlFPd/GzmMfb1D29fgq\nfwsOK/YAcBHWD03laa1biw3Atdhc6FH8sXJrROysKbdpfd000Wmddewpeq9jW7Fd5xZ8ergUr59P\nTFG+JnUfX6BPvFfgm6JH8QZ3XcO6p12eQqY6/VKZDct+Eb5dWI+dwHYXZdbqz+KDbSmeu/vxOr0M\n669OHZ20v8envtdJ6nxUTUXP99UthR7YWNQxhs06tle0QU+Kvn4fjkazH0cHWV+RdD4tb11Vf3My\nd5BjFu4Bzoj6v9yTTBOSJnDYoJ11aZMXN8UJ8zhwX0R8YbblOZmQ4ym/PCKWz7YsSZIMLyeTnp9t\nx7OZYFj+QtKcQLbP+WducOcmshPWUvyV/lIcw3EEf/UnSZIkc5yTWc/PdgixmWB4jqbnABFxMCLa\nGpknLx5ewFe6Yzho/nk4QHc/h54kSZJk7nDS6vmhMldIkiRJkiRJEhjOk9wkSZIkSZLkJCc3uUmS\nJEmSJMnQkZvcJEmSJEmSZOjITW6SJEmSJEkydOQmN0mSJEmSJBk6cpObJEmSJEmSDB25yU2SJEmS\nJEmGjtzkJkmSJEmSJEPH/wBzvIP89Wm9JgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d1d441470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_f = 2\n",
    "signal = stx[np.where(y == 1), idx_f + 1]\n",
    "background = stx[np.where(y == 0), idx_f + 1]\n",
    "\n",
    "plot = plt.figure()\n",
    "plt.boxplot([signal, background], 0, '')\n",
    "plt.xticks([1, 2], ['Signal', 'Background'])\n",
    "plot.suptitle('Signal and Background distribution of Feature {f}'.format(f = idx_f + 1))\n",
    "\n",
    "textvar = plot.text(0, 0, 'If the plot is similar, it means the signal does not provide more information than the background.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "gamma = 1e-6\n",
    "lambda_ = 0.001\n",
    "max_iters = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Prediction estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def divide_training_data(y, x, ratio ):\n",
    "    \"\"\"Divide a dataset into 2 disjoint parts. We will use this on the training data set \n",
    "    so that we can train on one and test on the other to check the accuracy of our prediction\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(np.arange(len(y)), int( len(y)*ratio ), replace=False)\n",
    "    \n",
    "    # training data \n",
    "    x_train = x[indices]\n",
    "    y_train = y[indices]\n",
    "    \n",
    "    # test data \n",
    "    x_test = x[~indices]\n",
    "    y_test = y[~indices]\n",
    "    \n",
    "\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prediction( y, tX, gamma, max_iters, lambda_  ):\n",
    "    # divide data\n",
    "    x_train, x_test, y_train, y_test = divide_training_data(y, tX, 0.5 )\n",
    "    \n",
    "    loss = 0\n",
    "    w = []\n",
    "    loss, w = logistic_regression(y_train, x_train, gamma, max_iters)\n",
    "    \n",
    "    y_pred = predict_labels(w, x_test)    \n",
    "    y_pred = set_y(y_pred)\n",
    "    \n",
    "    # accuracy of the prediction\n",
    "    N = y_test.shape\n",
    "    print(N, y_pred.shape)\n",
    "    pred = np.sum(y_pred == y_test)/N[0]\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=[[ 86643.39756999]]\n",
      "(125000,) (125000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75096799999999997"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(y, stx, 0.00001, 200, 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - tx @ w\n",
    "    return -(tx.T @ e)/len(y)\n",
    "\n",
    "def linear_regression_GD(y, tx, gamma, max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    ws = [np.copy(w)]\n",
    "    losses = []\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient with mse\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        \n",
    "        # update w\n",
    "        w = w - (gamma*grad)\n",
    "        \n",
    "        # calculate the cost with mse\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    print(\"Optimal weights: {w}\".format(w=ws[tx.shape[1]]), \"\\n Loss: {loss}\".format(loss = loss))\n",
    "    return ( loss, w )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss for Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: [  1.04648969e-02   3.75362625e-04  -4.99656749e-03  -1.91176313e-04\n",
      "   2.60325480e-03   2.83333842e-03   2.92009580e-03  -2.53973496e-03\n",
      "   2.92672884e-04  -2.91997173e-04   2.02535218e-03  -2.78308290e-03\n",
      "   3.80261289e-03   2.46044555e-03   3.29369359e-03  -1.44874300e-05\n",
      "  -6.42817694e-05  -4.79514601e-04   1.95107674e-05   5.97363034e-05\n",
      "   2.21582085e-04   1.05131123e-04   1.77564564e-03   1.77070788e-03\n",
      "   1.50527857e-03   9.97254502e-07   1.20346588e-05   2.19947304e-04\n",
      "   8.90938540e-06  -4.90302043e-05   1.74571674e-03] \n",
      " Loss: 0.15160026197005938\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.001\n",
    "max_iters = 100\n",
    "losses, ws = linear_regression_GD(y, stx, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_regression_SGD(y, tx, gamma, max_iters):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = np.zeros(tx.shape[1])\n",
    "    ws = [np.copy(w)]\n",
    "    losses = []\n",
    "    batch_size = 50000\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient for each ini batch\n",
    "        a = 0;\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches=1):\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            \n",
    "            # upgrade w\n",
    "            w = w - (gamma*grad)\n",
    "\n",
    "            # compute loss with mse\n",
    "            loss = compute_loss(y, tx, w)\n",
    "\n",
    "            # store w and loss\n",
    "            ws.append(np.copy(w))\n",
    "            losses.append(loss)\n",
    "    print(\"Optimal weights: {w}\\n\\n Loss: {l}\".format(w=ws[tx.shape[1]], l = loss))\n",
    "    return loss, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: [  1.04705105e-02   3.79845674e-04  -4.97651176e-03  -1.94235207e-04\n",
      "   2.63344112e-03   2.81190678e-03   2.90083034e-03  -2.52081839e-03\n",
      "   2.80011928e-04  -2.86502247e-04   2.03850299e-03  -2.76886684e-03\n",
      "   3.82063932e-03   2.45931692e-03   3.29498777e-03  -2.14715757e-05\n",
      "  -6.96247980e-05  -4.62731675e-04   2.84074841e-05   6.06792421e-05\n",
      "   2.59777250e-04   1.27353977e-04   1.78733661e-03   1.77304161e-03\n",
      "   1.52269183e-03   1.33327211e-05   7.02572929e-06   2.16524382e-04\n",
      "   4.10631762e-06  -6.96207410e-05   1.75716700e-03]\n",
      "\n",
      " Loss: 0.15160312268989676\n"
     ]
    }
   ],
   "source": [
    "losses, ws = linear_regression_SGD(y, stx, gamma, max_iters)\n",
    "w_LSSGD = ws[-1]\n",
    "w_LSSGD = w_LSSGD[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    w = np.linalg.solve(tx.T @ tx, tx.T @ y)\n",
    "    return w, compute_loss(y, tx, w)\n",
    "\n",
    "least_squares(y, stx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    w = np.linalg.solve((tx.T @ tx) + lamb*np.eye(tx.shape[1]), tx.T @ y)\n",
    "    return w, compute_loss(y, tx, w)\n",
    "\n",
    "lamb = 0\n",
    "weights, loss = ridge_regression(y, stx, lamb)\n",
    "weights = weights[1:]\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    # equivalent to use 1/(1+exp(-t)) but avoids overflow\n",
    "    return np.exp(-np.logaddexp(0, -t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    #for n in range(N):\n",
    "    #   cost += np.log(1+np.exp(tx[n, :].T @ w)) - (y[n] * tx[n, :].T @ w)\n",
    "    y = y.reshape((-1, 1))\n",
    "    return np.sum(np.logaddexp(0, tx @ w)) - y.T @ (tx @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    sig = sigmoid(tx @ w)\n",
    "    sig = sig.reshape(sig.shape[0],)\n",
    "    return tx.T @ (sig - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_* (w.T @ w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    \n",
    "    w.shape = (w.shape[0],)\n",
    "    w = w - gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, gamma, max_iters):\n",
    "    \"\"\"\n",
    "    Logistic regression using GD\n",
    "    \"\"\"\n",
    "    # init parameters\n",
    "    threshold = 0.001\n",
    "    losses = []\n",
    "\n",
    "    # build w\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    # y must be 0 or 1 and not -1 or 1(as implemented in the lab)\n",
    "    y = np.where(y == -1, 0, y)\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma, lambda_ = 0)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria ( max_iters is really high)\n",
    "        losses.append(np.copy(loss))\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters = 10000\n",
    "loss_LR, w_LR = logistic_regression(y, stx, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized  logistic  regression  using  gradient  descent or SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, gamma, max_iters):\n",
    "    \"\"\"\n",
    "    Logistic regression using GD\n",
    "    \"\"\"\n",
    "    # init parameters\n",
    "    threshold = 0.01\n",
    "    losses = []\n",
    "\n",
    "    # build w\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 500 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria ( max_iters is really high)\n",
    "        losses.append(np.copy(loss))\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w\n",
    "\n",
    "reg_logistic_regression(y, stx, lambda_, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the minimum of rmse_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cross_validation import *\n",
    "seed = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-4, 2, 30)\n",
    "cross_validation_demo(y, stx, k_fold, lambdas, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../../Data-Project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stx_test, mean_stx_test, std_x_test = standardize(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../../Data-Project1/Data_submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_LR, stx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
